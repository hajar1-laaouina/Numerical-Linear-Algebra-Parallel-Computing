# -*- coding: utf-8 -*-
"""mpi assignment.IPYNB

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12FGAcoT94E-yCFWxPJb5pA7Qsfd4cmdu
"""

!pip install mpi4py

# Commented out IPython magic to ensure Python compatibility.
# %%file hello.py
# from mpi4py import MPI
# COMM = MPI.COMM_WORLD
# SIZE = COMM.Get_size()
# RANK = COMM.Get_rank()
# 
# print("Hello from the rank {} process on {} total process".format(RANK,SIZE))

# Commented out IPython magic to ensure Python compatibility.
# %%file hello.py
# from mpi4py import MPI
# COMM = MPI.COMM_WORLD
# SIZE = COMM.Get_size()
# RANK = COMM.Get_rank()
# 
# if RANK == 0:
#     
#     print("Hello from the rank {} process on {} total process".format(RANK,SIZE))

!mpirun --oversubscribe -n 4 hello.py

from mpi4py import MPI

COMM = MPI.COMM_WORLD
RANK = COMM.Get_rank()
SIZE = COMM.Get_size()

print("hello from process ", RANK, " out of ", SIZE, " processes")

!mpirun --oversubscribe -n 4 python3 hello.py

"""exercice2

"""

# Commented out IPython magic to ensure Python compatibility.
# %%file helooo.py
# from mpi4py import MPI
# COMM = MPI.COMM_WORLD
# SIZE = COMM.Get_size()
# RANK = COMM.Get_rank()
# 
# if RANK==0:
#     sendb=int(input("Entrez un nombre: "))
# else:
#     sendb = None
#         
# recvb = COMM.bcast(sendb , root=0)
# print("Process {RANK} got {data}".format(RANK, data=recvb))

from mpi4py import MPI

COMM = MPI.COMM_WORLD
RANK = COMM.Get_rank()
SIZE = COMM.Get_size()

while True:
    if RANK == 0:
        num = int(input("Enter an integer: "))
    else:
        num = None
    num = COMM.bcast(num, root=0)
    if num < 0:
        break
    print("Process %d got %d" % (RANK, num))
    COMM.Barrier()

"""EXERCICE3

"""

# Commented out IPython magic to ensure Python compatibility.
# %%file rank.py
# from mpi4py import MPI
# COMM = MPI.COMM_WORLD
# nbOfproc = COMM.Get_size()
# RANK = COMM.Get_rank()
# tag=99
# i=0
# while i < nbOfproc-1:
#     if RANK==i:
#         sendb = 1000
#         COMM.send ( sendb , dest=i+1, tag=tag )
#     if RANK ==i+1:
#         recvb = COMM.recv ( source=i , tag=tag )
#         print ("Process {RANK} receive {recvb} from {RANKO}".format(RANK=RANK, recvb=recvb, RANKO=RANK-1))
#     i=i+1

from mpi4py import MPI

comm = MPI.COMM_WORLD
size = comm.Get_size()
rank = comm.Get_rank()

# Process 0 reads input from the user
if rank == 0:
    data = int(input("Enter an integer: "))
else:
    data = None

# Send the data around the ring
for i in range(size):
    if i == rank:
        print(f"Process {rank} received {data}")
        data = (data + rank) if data is not None else None
        dest = (rank + 1) % size
        comm.send(data, dest=dest)
    else:
        source = (rank - 1) % size
        data = comm.recv(source=source)
        print(f"Process {rank} received {data}")
        data = (data + rank) if data is not None else None
        dest = (rank + 1) % size
        comm.send(data, dest=dest)

"""Exercice4"""

# Commented out IPython magic to ensure Python compatibility.
# %%file scatter.py
# from mpi4py import MPI
# 
# COLS = 8
# ROWS = 8
# 
# comm = MPI.COMM_WORLD
# p = comm.Get_size()
# rank = comm.Get_rank()
# 
# a = None
# 
# NPROWS = 2
# NPCOLS = 2
# BLOCKROWS = ROWS // NPROWS
# BLOCKCOLS = COLS // NPCOLS
# 
# if rank == 0:
#     a = bytearray(range(ROWS*COLS))
# 
# if p != NPROWS*NPCOLS:
#     if rank == 0:
#         print(f"Error: number of PEs {p} != {NPROWS} x {NPCOLS}")
#     comm.Abort()
# 
# b = bytearray(BLOCKROWS * BLOCKCOLS)
# 
# blocktype2 = MPI.CHAR.Create_vector(BLOCKROWS, BLOCKCOLS, COLS)
# blocktype2.Commit()
# 
# blocktype = blocktype2.Create_resized(0, 1)
# blocktype.Commit()
# 
# disps = [i * COLS * BLOCKROWS + j * BLOCKCOLS for i in range(NPROWS) for j in range(NPCOLS)]
# counts = [1] * (NPROWS * NPCOLS)
# 
# comm.Scatterv([a, counts, disps, blocktype], b)
# 
# for proc in range(p):
#     if proc == rank:
#         print(f"Rank = {rank}")
#         if rank == 0:
#             print("Global matrix:")
#             for i in range(ROWS):
#                 for j in range(COLS):
#                     print(f"{a[i*COLS+j]:3d}", end=" ")
#                 print("")
#         print("Local Matrix:")
#         for i in range(BLOCKROWS):
#             for j in range(BLOCKCOLS):
#                 print(f"{b[i*BLOCKCOLS+j]:3d}", end=" ")
#             print("")
#         print("")
# 
#     comm.Barrier()
# 
# blocktype2.Free()
# blocktype.Free()
# 
# MPI.Finalize()

from mpi4py import MPI

COLS = 16
ROWS = 16

comm = MPI.COMM_WORLD
p = comm.Get_size()
rank = comm.Get_rank()

a = None

NPROWS = 2
NPCOLS = 2
BLOCKROWS = ROWS // NPROWS
BLOCKCOLS = COLS // NPCOLS

if rank == 0:
    a = bytearray(range(ROWS*COLS))

if p != NPROWS*NPCOLS:
    if rank == 0:
        print(f"Error: number of PEs {p} != {NPROWS} x {NPCOLS}")
    comm.Abort()

b = bytearray(BLOCKROWS * BLOCKCOLS)

blocktype2 = MPI.CHAR.Create_vector(BLOCKROWS, BLOCKCOLS, COLS)
blocktype2.Commit()

blocktype = blocktype2.Create_resized(0, 1)
blocktype.Commit()

disps = [i * COLS * BLOCKROWS + j * BLOCKCOLS for i in range(NPROWS) for j in range(NPCOLS)]
counts = [1] * (NPROWS * NPCOLS)

comm.Scatterv([a, counts, disps, blocktype], b)

for proc in range(p):
    if proc == rank:
        print(f"Rank = {rank}")
        if rank == 0:
            print("Global matrix:")
            for i in range(ROWS):
                for j in range(COLS):
                    print(f"{a[i*COLS+j]:3d}", end=" ")
                print("")
        print("Local Matrix:")
        for i in range(BLOCKROWS):
            for j in range(BLOCKCOLS):
                print(f"{b[i*BLOCKCOLS+j]:3d}", end=" ")
            print("")
        print("")

    comm.Barrier()

blocktype2.Free()
blocktype.Free()

MPI.Finalize()

"""exercice5

"""

# Commented out IPython magic to ensure Python compatibility.
# %%file MatrixVectorMulti.py
# 
# import numpy as np
# from scipy.sparse import lil_matrix
# from numpy.random import rand, seed
# from numba import njit
# from mpi4py import MPI
# 
# 
# ''' This program compute parallel csc matrix vector multiplication using mpi '''
# 
# COMM = MPI.COMM_WORLD
# nbOfproc = COMM.Get_size()
# RANK = COMM.Get_rank()
# 
# seed(42)
# 
# def matrixVectorMult(A, b, x):
#     
#     row, col = A.shape
#     for i in range(row):
#         a = A[i]
#         for j in range(col):
#             x[i] += a[j] * b[j]
# 
#     return 0
# 
# ########################initialize matrix A and vector b ######################
# #matrix sizes
# SIZE = 1000
# Local_size = SIZE//nbOfproc
# 
# # counts = block of each proc
# #counts = 
# 
# if RANK == 0:
#     A = lil_matrix((SIZE, SIZE))
#     A[0, :100] = rand(100)
#     A[1, 100:200] = A[0, :100]
#     A.setdiag(rand(SIZE))
#     A = A.toarray()
#     b = rand(SIZE)
# else :
#     A = None
#     b = None
# 
# 
# 
# #########Send b to all procs and scatter A (each proc has its own local matrix#####
# 
# LocalMatrix = np.zeros((Local_size, SIZE))
# COMM.Scatter(A, LocalMatrix, root=0)
# 
# # Scatter the matrix A
# b=COMM.bcast(b,root=0)
# 
# 
# 
# #####################Compute A*b locally#######################################
# LocalX = np.zeros(Local_size)
# 
# 
# start = MPI.Wtime()
# matrixVectorMult(LocalMatrix, b, LocalX)
# stop = MPI.Wtime()
# if RANK == 0:
#     print("CPU time of parallel multiplication is ", (stop - start)*1000)
# 
# ##################Gather te results ###########################################
# #sendcouns = 
# sendcounts = np.array(COMM.gather(len(LocalX),root=0))
# if RANK == 0: 
#     X = np.zeros(sum(sendcounts),dtype=np.double)
# 
# else :
#     X = None
# 
# if RANK == 0:
#     print(len(X))
# #print(RANK, sendcounts, len(LocalX))
# # Gather the result into X
# COMM.Gatherv(LocalX, recvbuf=(X, sendcounts, MPI.DOUBLE), root=0)
# 
# 
# ##################Print the results ###########################################
# 
# if RANK == 0 :
#     X_ = A.dot(b)
#     print("The result of A*b using dot is :", np.max(X_ - X))
#     # print("The result of A*b using parallel version is :", X)
#

import numpy as np
from scipy.sparse import lil_matrix
from numpy.random import rand, seed
from numba import njit
from mpi4py import MPI


''' This program compute parallel csc matrix vector multiplication using mpi '''

COMM = MPI.COMM_WORLD
nbOfproc = COMM.Get_size()
RANK = COMM.Get_rank()

seed(42)

def matrixVectorMult(A, b, x):
    
    row, col = A.shape
    for i in range(row):
        a = A[i]
        for j in range(col):
            x[i] += a[j] * b[j]

    return 0

########################initialize matrix A and vector b ######################
#matrix sizes
SIZE = 1000
Local_size = SIZE // nbOfproc

counts = [Local_size if i < nbOfproc - 1 else SIZE - (nbOfproc - 1) * Local_size for i in range(nbOfproc)]
displs = [sum(counts[:i]) for i in range(nbOfproc)]

if RANK == 0:
    A = lil_matrix((SIZE, SIZE))
    A[0, :100] = rand(100)
    A[1, 100:200] = A[0, :100]
    A.setdiag(rand(SIZE))
    A = A.toarray()
    b = rand(SIZE)
else :
    A = None
    b = None

b = COMM.bcast(b, root=0)
LocalMatrix = np.zeros((counts[RANK], SIZE))
COMM.Scatterv([A, counts, displs, MPI.DOUBLE], LocalMatrix, root=0)

#####################Compute A*b locally#######################################
LocalX = np.zeros(counts[RANK])
start = MPI.Wtime()
matrixVectorMult(LocalMatrix, b, LocalX)
stop = MPI.Wtime()
if RANK == 0:
    print("CPU time of parallel multiplication is ", (stop - start)*1000)

##################Gather the results ###########################################
sendcounts = counts[RANK]
X = None
if RANK == 0: 
    X = np.zeros(SIZE)
    recvcounts = [counts[i] for i in range(nbOfproc)]
    recvdispls = [sum(counts[:i]) for i in range(nbOfproc)]
else :
    recvcounts = None
    recvdispls = None
COMM.Gatherv(LocalX, [X, sendcounts, recvdispls, MPI.DOUBLE], root=0)

##################Print the results ###########################################

if RANK == 0 :
    X_ = A.dot(b)
    print("The result of A*b using dot is :", np.max(X_ - X))
    # print("The result of A*b using parallel version is :", X)

import matplotlib.pyplot as plt
CPU_time=[553.423, 643.515, 236.962]
plt.plot([1,2,4],CPU_time)
plt.title("CPU time per number of processes use to the product Matrix vect")

"""EXERCICE6

"""

# Commented out IPython magic to ensure Python compatibility.
# %%file Pim.py
# 
# import random 
# import timeit
# from mpi4py import MPI
# 
# COMM = MPI.COMM_WORLD
# nbOfproc = COMM.Get_size()
# RANK = COMM.Get_rank()
# 
# 
# 
# random.seed(42)  
# 
# def compute_points(n):
#     
#     random.seed(42)  
#     
#     circle_points= 0
# 
#     # Total Random numbers generated= possible x 
#     # values* possible y values 
#     for i in range(n): 
#       
#         # Randomly generated x and y values from a 
#         # uniform distribution 
#         # Rannge of x and y values is -1 to 1 
#                 
#         rand_x= random.uniform(-1, 1) 
#         rand_y= random.uniform(-1, 1) 
#       
#         # Distance between (x, y) from the origin 
#         origin_dist= rand_x**2 + rand_y**2
#       
#         # Checking if (x, y) lies inside the circle 
#         if origin_dist<= 1: 
#             circle_points+= 1
#       
#         # Estimating value of pi, 
#         # pi= 4*(no. of points generated inside the  
#         # circle)/ (no. of points generated inside the square) 
#     
#      
#     
#     return circle_points
# 
# INTERVAL= 1000
# a= int((INTERVAL**2)/4)
# start = timeit.default_timer()
# circle_points = compute_points(a)
# end = timeit.default_timer()
# pi = 4* circle_points/ INTERVAL**2 
# circle_point_reduced = COMM.reduce(circle_points, op=MPI.SUM, root=0)
# pi_reduced = COMM.reduce(pi, op=MPI.SUM, root=0)
# if RANK==0:
#     print("Circle points number :",circle_point_reduced )
#     print("Final Estimation of Pi=", pi_reduced, "cpu time :",end-start)

import random 
import timeit
from mpi4py import MPI

COMM = MPI.COMM_WORLD
nbOfproc = COMM.Get_size()
RANK = COMM.Get_rank()



random.seed(42)  

def compute_points(n):
    
    random.seed(42)  
    
    circle_points= 0

    # Total Random numbers generated= possible x 
    # values* possible y values 
    for i in range(n): 
      
        # Randomly generated x and y values from a 
        # uniform distribution 
        # Rannge of x and y values is -1 to 1 
                
        rand_x= random.uniform(-1, 1) 
        rand_y= random.uniform(-1, 1) 
      
        # Distance between (x, y) from the origin 
        origin_dist= rand_x**2 + rand_y**2
      
        # Checking if (x, y) lies inside the circle 
        if origin_dist<= 1: 
            circle_points+= 1
      
        # Estimating value of pi, 
        # pi= 4*(no. of points generated inside the  
        # circle)/ (no. of points generated inside the square) 
    
     
    
    return circle_points

INTERVAL= 1000
a= int((INTERVAL**2)/4)
start = timeit.default_timer()
circle_points = compute_points(a)
end = timeit.default_timer()
pi = 4* circle_points/ INTERVAL**2 
circle_point_reduced = COMM.reduce(circle_points, op=MPI.SUM, root=0)
pi_reduced = COMM.reduce(pi, op=MPI.SUM, root=0)
if RANK==0:
    print("Circle points number :",circle_point_reduced )
    print("Final Estimation of Pi=", pi_reduced, "cpu time :",end-start)

CPU_time=[0.20, 0.25, 0.27]
plt.plot([1, 2, 4],CPU_time)
plt.title("CPU time per number of processes use to the Pi MonteCarlo ")

